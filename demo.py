#!/usr/bin/env python3
"""
SubForge Demo - Complete End-to-End Demonstration
Shows all SubForge capabilities in action
"""

import asyncio
from pathlib import Path

from subforge.core.project_analyzer import ProjectAnalyzer
from subforge.core.workflow_orchestrator import WorkflowOrchestrator
from subforge.core.validation_engine import ValidationEngine
from subforge.core.communication import CommunicationChannel, send_analysis_report


def print_section(title: str, emoji: str = "ðŸ”¥"):
    """Print a demo section header"""
    print(f"\n{emoji} " + "=" * 50)
    print(f" {title}")
    print("=" * 52)


async def main():
    """Complete SubForge demonstration"""

    print(r"""
 ____        _     _____
/ ___| _   _| |__ |  ___|__  _ __ __ _  ___
\___ \| | | | '_ \| |_ / _ \| '__/ _` |/ _ \
 ___) | |_| | |_) |  _| (_) | | | (_| |  __/
|____/ \__,_|_.__/|_|  \___/|_|  \__, |\___|
                                 |___/

ðŸš€ SubForge v1.0-Alpha - Complete Demonstration
""")

    # Demo project path
    demo_path = Path.cwd()

    print_section("1. PROJECT ANALYSIS", "ðŸ”")
    print(f"Analyzing project: {demo_path.name}")

    # Initialize components
    analyzer = ProjectAnalyzer()
    orchestrator = WorkflowOrchestrator()
    validator = ValidationEngine()

    # Run project analysis
    profile = await analyzer.analyze_project(str(demo_path))

    print("\nðŸ“Š Analysis Results:")
    print(f"  â€¢ Project: {profile.name}")
    print(f"  â€¢ Architecture: {profile.architecture_pattern.value}")
    print(f"  â€¢ Complexity: {profile.complexity.value}")
    print(f"  â€¢ Languages: {', '.join(profile.technology_stack.languages)}")
    print(f"  â€¢ Recommended Team: {len(profile.recommended_subagents)} subagents")

    for agent in profile.recommended_subagents:
        print(f"    - {agent}")

    print_section("2. WORKFLOW ORCHESTRATION", "âš¡")
    print("Executing complete SubForge workflow...")

    # Run full workflow
    user_request = "Create a comprehensive development environment with modern tooling and best practices"
    context = await orchestrator.execute_workflow(user_request, str(demo_path))

    print("\nðŸŽ¯ Workflow Results:")
    print(f"  â€¢ Workflow ID: {context.project_id}")
    print(f"  â€¢ Phases Completed: {len(context.phase_results)}")
    print(f"  â€¢ Generated Subagents: {len(context.template_selections.get('selected_templates', []))}")
    print(f"  â€¢ Configuration Files: {len(list(context.communication_dir.glob('**/*.md')))}")

    print_section("3. COMMUNICATION SYSTEM", "ðŸ’¬")
    print("Testing markdown-based communication...")

    # Create communication channel
    channel = CommunicationChannel(context.communication_dir / "communication_test")

    # Send analysis report
    message_id = send_analysis_report(
        channel,
        "project-analyzer",
        profile.to_dict()
    )

    # Get messages
    summary = channel.get_channel_summary()

    print("\nðŸ“¨ Communication Results:")
    print(f"  â€¢ Message ID: {message_id}")
    print(f"  â€¢ Total Messages: {summary['total_messages']}")
    print(f"  â€¢ Message Types: {list(summary['message_types'].keys())}")

    print_section("4. VALIDATION ENGINE", "âœ…")
    print("Running comprehensive validation...")

    # Prepare validation context
    validation_config = {
        "project_name": profile.name,
        "subagents": context.template_selections.get("selected_templates", [])
    }

    validation_context = {
        "project_id": context.project_id,
        "project_profile": profile.to_dict(),
        "template_selections": context.template_selections,
        "generated_configurations": context.generated_configurations
    }

    # Run validation
    report = validator.validate_configuration(validation_config, validation_context)

    print("\nðŸŽ¯ Validation Results:")
    print(f"  â€¢ Overall Score: {report.overall_score:.1f}%")
    print(f"  â€¢ Deployment Ready: {'âœ… Yes' if report.deployment_ready else 'âŒ No'}")
    print(f"  â€¢ Total Checks: {report.total_checks}")
    print(f"  â€¢ Passed: {report.passed_checks}")
    print(f"  â€¢ Warnings: {report.warning_checks}")
    print(f"  â€¢ Critical Issues: {report.critical_issues}")

    if report.recommendations:
        print("\nðŸ’¡ Top Recommendations:")
        for rec in report.recommendations[:3]:
            print(f"  â€¢ {rec}")

    # Test deployment
    success, message = validator.create_test_deployment(validation_config, validation_context)
    print(f"\nðŸ§ª Test Deployment: {'âœ… Success' if success else 'âŒ Failed'}")
    print(f"  â€¢ {message}")

    print_section("5. GENERATED ARTIFACTS", "ðŸ“")
    print("Exploring generated configuration files...")

    # List generated files
    total_files = 0
    for file_type in ["*.md", "*.json", "*.yaml"]:
        files = list(context.communication_dir.glob(f"**/{file_type}"))
        if files:
            print(f"\nðŸ“„ {file_type.replace('*', '').upper()} Files ({len(files)}):")
            for file_path in files[:5]:  # Show first 5
                relative_path = file_path.relative_to(context.communication_dir)
                size = file_path.stat().st_size
                print(f"  â€¢ {relative_path} ({size:,} bytes)")
            if len(files) > 5:
                print(f"  â€¢ ... and {len(files) - 5} more")
            total_files += len(files)

    print(f"\nðŸ“Š Total Generated Files: {total_files}")

    print_section("6. PERFORMANCE METRICS", "âš¡")

    # Calculate performance metrics
    total_duration = sum(result.duration for result in context.phase_results.values())

    print("Performance Summary:")
    print(f"  â€¢ Total Execution Time: {total_duration:.2f} seconds")
    print(f"  â€¢ Files Analyzed: {profile.file_count:,}")
    print(f"  â€¢ Lines Processed: {profile.lines_of_code:,}")
    print(f"  â€¢ Configuration Files Generated: {total_files}")
    print(f"  â€¢ Validation Checks: {report.total_checks}")

    if total_duration > 0:
        files_per_sec = profile.file_count / total_duration
        lines_per_sec = profile.lines_of_code / total_duration
        print(f"  â€¢ Processing Speed: {files_per_sec:.1f} files/sec, {lines_per_sec:,.0f} lines/sec")

    print_section("7. DEMO COMPLETE! ðŸŽ‰", "ðŸš€")

    print(f"""
âœ¨ SubForge v1.0-Alpha Demonstration Complete!

ðŸŽ¯ What We Just Did:
  â€¢ Analyzed a real codebase ({profile.file_count:,} files, {profile.lines_of_code:,} lines)
  â€¢ Orchestrated a 7-phase workflow with parallel processing
  â€¢ Generated {len(context.template_selections.get('selected_templates', []))} specialized subagent configurations
  â€¢ Created {total_files} configuration files with markdown communication
  â€¢ Ran {report.total_checks} comprehensive validation checks
  â€¢ Achieved {report.overall_score:.1f}% quality score with deployment readiness
  â€¢ Completed everything in {total_duration:.2f} seconds

ðŸš€ SubForge is now ready for:
  â€¢ Beta testing with real projects
  â€¢ Community template contributions
  â€¢ GitHub repository publication
  â€¢ Claude Code marketplace integration

Configuration saved in: {context.communication_dir}
    """)


if __name__ == "__main__":
    asyncio.run(main())
